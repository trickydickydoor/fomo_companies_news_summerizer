from typing import List, Dict, Any
from src.services.supabase_service import SupabaseService
from src.services.gemini_service import GeminiService
from src.services.pinecone_service import PineconeService

class NewsAnalyzer:
    def __init__(self, debug=False):
        self.debug = debug
        self.supabase_service = SupabaseService()
        self.gemini_service = GeminiService()
        self.pinecone_service = PineconeService()
    
    def _debug_print(self, message, data=None):
        """è°ƒè¯•è¾“å‡ºå‡½æ•°"""
        if self.debug:
            print(f"ğŸ” DEBUG: {message}", flush=True)
            if data is not None:
                if isinstance(data, (list, dict)):
                    import json
                    print(f"    æ•°æ®: {json.dumps(data, ensure_ascii=False, indent=2)}")
                else:
                    print(f"    æ•°æ®: {str(data)}")
            print("    " + "-" * 50, flush=True)
    
    def analyze_all_companies(self, hours: int = 24) -> List[Dict[str, Any]]:
        """
        åˆ†ææ‰€æœ‰å…¬å¸çš„æœ€æ–°æ–°é—»
        
        Args:
            hours: åˆ†ææ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            List[Dict]: æ‰€æœ‰å…¬å¸çš„åˆ†æç»“æœ
        """
        print("æ­£åœ¨è·å–å…¬å¸åˆ—è¡¨...")
        companies = self.supabase_service.get_companies()
        
        if not companies:
            print("æœªæ‰¾åˆ°ä»»ä½•å…¬å¸æ•°æ®")
            return []
        
        print(f"æ‰¾åˆ° {len(companies)} å®¶å…¬å¸ï¼Œæ£€æŸ¥æ–‡ç« è®¡æ•°å˜åŒ–...")
        
        results = []
        analyzed_count = 0
        skipped_count = 0
        
        for company in companies:
            company_id = company.get('id', '')
            company_name = company.get('name', 'æœªçŸ¥å…¬å¸')
            
            print(f"\nğŸ” æ£€æŸ¥å…¬å¸: {company_name}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†æè¯¥å…¬å¸
            should_analyze, current_article_count = self.supabase_service.should_analyze_company(company_id)
            
            if should_analyze:
                print(f"âœ… {company_name} æ–‡ç« æ•°é‡æœ‰å˜åŒ–ï¼Œå¼€å§‹åˆ†æ...")
                analysis_result = self.analyze_single_company_with_count_update(
                    company_name, company_id, current_article_count, hours
                )
                if analysis_result:
                    results.append(analysis_result)
                    analyzed_count += 1
            else:
                print(f"â­ï¸  {company_name} æ–‡ç« æ•°é‡æ— å˜åŒ–ï¼Œè·³è¿‡åˆ†æ")
                skipped_count += 1
        
        print(f"\nğŸ“Š åˆ†ææ€»ç»“: åˆ†æäº† {analyzed_count} å®¶å…¬å¸ï¼Œè·³è¿‡ {skipped_count} å®¶å…¬å¸")
        return results
    
    def analyze_single_company_with_count_update(self, company_name: str, company_id: str, current_article_count: int, hours: int = 24) -> Dict[str, Any]:
        """
        åˆ†æå•ä¸ªå…¬å¸çš„æœ€æ–°æ–°é—»ï¼Œå¹¶åœ¨å®Œæˆåæ›´æ–°æ–‡ç« è®¡æ•°
        
        Args:
            company_name: å…¬å¸åç§°
            company_id: å…¬å¸ID
            current_article_count: å½“å‰æ–‡ç« æ•°é‡
            hours: åˆ†ææ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            Dict: åˆ†æç»“æœ
        """
        # å¦‚æœå½“å‰æ–‡ç« æ•°é‡ä¸º0ï¼Œç›´æ¥è¿”å›nullå†…å®¹ï¼Œä¸éœ€è¦åˆ†æ
        if current_article_count == 0:
            print(f"ğŸ“­ {company_name} å½“å‰24å°æ—¶å†…æ— æ–‡ç« ï¼Œè®¾ç½®å†…å®¹ä¸ºnull")
            
            # æ›´æ–°last_article_countä¸º0
            update_success = self.supabase_service.update_last_article_count(company_id, 0)
            if update_success:
                print(f"âœ… {company_name} æ–‡ç« è®¡æ•°æ›´æ–°ä¸º0")
            else:
                print(f"âš ï¸  {company_name} æ–‡ç« è®¡æ•°æ›´æ–°å¤±è´¥")
            
            # è¿”å›contentä¸ºnullçš„ç»“æœ
            return {
                'company': company_name,
                'news_count': 0,
                'analysis': None,  # contentè®¾ç½®ä¸ºnull
                'sources': [],
                'time_range_hours': hours,
                'status': 'no_news',
                'message': 'å½“å‰24å°æ—¶å†…æ— æ–‡ç« '
            }
        
        # æ‰§è¡Œåˆ†æ
        analysis_result = self.analyze_single_company(company_name, hours)
        
        # å¦‚æœåˆ†ææˆåŠŸï¼Œæ›´æ–°last_article_count
        if analysis_result and analysis_result.get('status') in ['success', 'no_news', 'no_vector_data']:
            print(f"\nğŸ”„ åˆ†æå®Œæˆï¼Œæ›´æ–° {company_name} çš„æ–‡ç« è®¡æ•°...")
            update_success = self.supabase_service.update_last_article_count(company_id, current_article_count)
            if update_success:
                print(f"âœ… {company_name} æ–‡ç« è®¡æ•°æ›´æ–°æˆåŠŸ")
            else:
                print(f"âš ï¸  {company_name} æ–‡ç« è®¡æ•°æ›´æ–°å¤±è´¥")
        
        return analysis_result
    
    def analyze_single_company(self, company_name: str, hours: int = 24) -> Dict[str, Any]:
        """
        åˆ†æå•ä¸ªå…¬å¸çš„æœ€æ–°æ–°é—»
        
        Args:
            company_name: å…¬å¸åç§°
            hours: åˆ†ææ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            Dict: åˆ†æç»“æœ
        """
        try:
            print(f"\nğŸ“ˆ å¼€å§‹åˆ†æå…¬å¸: {company_name}", flush=True)
            print("=" * 60, flush=True)
            
            # æ­¥éª¤1: ä»Supabaseè·å–ç›¸å…³æ–°é—»ID
            print(f"ğŸ“Š æ­¥éª¤1: è·å– {company_name} æœ€è¿‘ {hours} å°æ—¶çš„æ–°é—»ID...", flush=True)
            news_ids = self.supabase_service.get_company_news_ids(company_name, hours)
            
            self._debug_print(f"ä»Supabaseè·å–çš„æ–°é—»ID", news_ids)
            
            if not news_ids:
                print(f"âš ï¸  {company_name} æœªæ‰¾åˆ°ç›¸å…³æ–°é—»")
                return {
                    'company': company_name,
                    'news_count': 0,
                    'analysis': None,  # è¿”å›nullè€Œéæ–‡æœ¬
                    'sources': [],
                    'time_range_hours': hours,
                    'status': 'no_news'
                }
            
            print(f"âœ… æ‰¾åˆ° {len(news_ids)} æ¡ç›¸å…³æ–°é—»")
            
            # æ­¥éª¤2: ç”ŸæˆæŸ¥è¯¢è¯­å¥å’Œå‘é‡
            print(f"\nğŸ§  æ­¥éª¤2: ç”Ÿæˆè¯­ä¹‰æœç´¢æŸ¥è¯¢...")
            query_text = self.gemini_service.generate_company_query(company_name)
            print(f"ğŸ“ æŸ¥è¯¢è¯­å¥: {query_text}")
            
            self._debug_print("ç”Ÿæˆçš„æŸ¥è¯¢æ–‡æœ¬", query_text)
            
            query_vector = self.gemini_service.generate_embedding(query_text)
            if not query_vector:
                print(f"âŒ ç”ŸæˆæŸ¥è¯¢å‘é‡å¤±è´¥")
                return None
            
            print(f"âœ… ç”Ÿæˆå‘é‡æˆåŠŸï¼Œç»´åº¦: {len(query_vector)}")
            self._debug_print("æŸ¥è¯¢å‘é‡æ ·æœ¬ (å‰10ä¸ªå€¼)", query_vector[:10])
            
            # æ­¥éª¤3: ä½¿ç”¨è¯­ä¹‰æœç´¢ç»“åˆmetadataè¿‡æ»¤åœ¨Pineconeä¸­æœç´¢
            print(f"\nğŸ” æ­¥éª¤3: ä½¿ç”¨è¯­ä¹‰æœç´¢åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢ç›¸å…³å†…å®¹...")
            news_data = self.pinecone_service.search_with_semantic_and_metadata(
                query_vector=query_vector,
                news_ids=news_ids,
                company_name=company_name,
                hours=hours,
                top_k=50
            )
            
            self._debug_print("Pineconeè¿”å›çš„åŒ¹é…ç»“æœ", [
                {
                    'id': item.get('id'),
                    'score': item.get('score'),
                    'metadata_keys': list(item.get('metadata', {}).keys())
                } for item in news_data
            ])
            
            if not news_data:
                print(f"âš ï¸  å‘é‡æ•°æ®åº“ä¸­æœªæ‰¾åˆ° {company_name} çš„ç›¸å…³å†…å®¹")
                return {
                    'company': company_name,
                    'news_count': len(news_ids),  # å®é™…æ‰¾åˆ°äº†æ–°é—»ID
                    'analysis': None,  # ä½†æ²¡æœ‰å‘é‡å†…å®¹ï¼Œè¿”å›null
                    'sources': [],
                    'time_range_hours': hours,
                    'status': 'no_vector_data',
                    'message': f"æ‰¾åˆ°{len(news_ids)}æ¡æ–°é—»è®°å½•ï¼Œä½†å‘é‡æ•°æ®åº“ä¸­æ— å¯¹åº”å†…å®¹"
                }
            
            print(f"âœ… ä»å‘é‡æ•°æ®åº“è·å–åˆ° {len(news_data)} æ¡æ–°é—»å†…å®¹")
            
            # æ­¥éª¤4: æ˜¾ç¤ºå…·ä½“çš„æ–°é—»å†…å®¹
            print(f"\nğŸ“° æ­¥éª¤4: æ£€ç´¢åˆ°çš„æ–°é—»å†…å®¹é¢„è§ˆ...")
            for i, news in enumerate(news_data[:3], 1):  # åªæ˜¾ç¤ºå‰3æ¡
                metadata = news.get('metadata', {})
                title = metadata.get('article_title', metadata.get('title', 'æ— æ ‡é¢˜'))[:50]
                score = news.get('score', 0)
                print(f"   {i}. {title}... (ç›¸ä¼¼åº¦: {score:.3f})")
            
            if len(news_data) > 3:
                print(f"   ... è¿˜æœ‰ {len(news_data) - 3} æ¡æ–°é—»")
            
            # æ­¥éª¤5: ä½¿ç”¨Geminiåˆ†ææ–°é—»å†…å®¹
            print(f"\nğŸ¤– æ­¥éª¤5: ä½¿ç”¨Geminiåˆ†ææ–°é—»å†…å®¹...")
            
            # æ˜¾ç¤ºå‘é€ç»™AIçš„å†…å®¹æ ·æœ¬
            sample_content = self.gemini_service._format_news_for_analysis(news_data[:2])
            self._debug_print("å‘é€ç»™Geminiåˆ†æçš„å†…å®¹æ ·æœ¬ï¼ˆå‰2æ¡æ–°é—»ï¼‰", sample_content)
            
            analysis_result = self.gemini_service.analyze_news(company_name, news_data)
            
            print(f"âœ… åˆ†æå®Œæˆï¼Œç”ŸæˆæŠ¥å‘Šé•¿åº¦: {len(analysis_result)} å­—ç¬¦")
            self._debug_print("Geminiåˆ†æç»“æœå®Œæ•´å†…å®¹", analysis_result)
            
            # æ­¥éª¤6: æå–å¼•ç”¨ä¿¡æ¯
            print(f"\nğŸ“š æ­¥éª¤6: æ•´ç†å¼•ç”¨ä¿¡æ¯...")
            sources = self._extract_sources(news_data)
            print(f"âœ… æ•´ç†äº† {len(sources)} ä¸ªæ–°é—»æ¥æº")
            
            print("\nğŸ‰ åˆ†æå®Œæˆï¼")
            print("=" * 60)
            
            # ç»Ÿä¸€è¿”å›æ ¼å¼
            return {
                'company': company_name,
                'news_count': len(news_ids),  # ä½¿ç”¨ä»Supabaseè·å–çš„æ–°é—»IDæ•°é‡
                'analysis': analysis_result,  # å¯èƒ½æ˜¯dictï¼ˆJSONæ ¼å¼ï¼‰æˆ–strï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
                'sources': sources,
                'time_range_hours': hours,
                'status': analysis_result.get('status', 'success') if isinstance(analysis_result, dict) else 'success'
            }
            
        except Exception as e:
            print(f"  åˆ†æ {company_name} æ—¶å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return {
                'company': company_name,
                'news_count': 0,
                'analysis': None,
                'sources': [],
                'time_range_hours': hours,
                'status': 'error',
                'error': str(e)
            }
    
    def _extract_sources(self, news_data: List[Dict[str, Any]]) -> List[Dict[str, str]]:
        """
        ä»æ–°é—»æ•°æ®ä¸­æå–å¼•ç”¨ä¿¡æ¯
        
        Args:
            news_data: æ–°é—»æ•°æ®åˆ—è¡¨
            
        Returns:
            List[Dict]: å¼•ç”¨ä¿¡æ¯åˆ—è¡¨
        """
        sources = []
        seen_urls = set()  # ç”¨äºå»é‡çš„URLé›†åˆ
        
        for news in news_data:
            metadata = news.get('metadata', {})
            url = metadata.get('article_url', metadata.get('url', ''))
            
            # è·³è¿‡å·²ç»è§è¿‡çš„URLæˆ–ç©ºURL
            if url in seen_urls or not url:
                continue
                
            source_info = {
                'news_id': metadata.get('news_id', news.get('id', '')),
                'title': metadata.get('article_title', metadata.get('title', 'æ— æ ‡é¢˜')),
                'source': metadata.get('source', 'æœªçŸ¥æ¥æº'),
                'published_at': metadata.get('article_published_time', metadata.get('published_at', 'æœªçŸ¥æ—¶é—´')),
                'url': url,
                'score': round(news.get('score', 0), 3)
            }
            sources.append(source_info)
            seen_urls.add(url)  # æ ‡è®°è¯¥URLå·²å¤„ç†
        
        # æŒ‰ç›¸ä¼¼åº¦åˆ†æ•°æ’åº
        sources.sort(key=lambda x: x['score'], reverse=True)
        return sources